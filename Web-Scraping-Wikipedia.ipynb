{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "# make a list of cities we want to do webscraping \n",
    "cities = ['Berlin','Hamburg','Munich','Cologne','Frankfurt']\n",
    "\n",
    "# a function to get city info\n",
    "def City_info(soup):\n",
    "    \n",
    "    ret_dict = {}\n",
    "    ret_dict['city'] = soup.h1.get_text()\n",
    "    \n",
    "    if soup.select_one('.mergedrow:-soup-contains(\"Mayor\")>.infobox-label') != None:\n",
    "        i = soup.select_one('.mergedrow:-soup-contains(\"Mayor\")>.infobox-label')\n",
    "        mayor_name_html = i.find_next_sibling()\n",
    "        mayor_name = unicodedata.normalize('NFKD',mayor_name_html.get_text())\n",
    "        ret_dict['mayor']  = mayor_name\n",
    "    \n",
    "    if soup.select_one('.mergedrow:-soup-contains(\"City\")>.infobox-label') != None:\n",
    "        j =  soup.select_one('.mergedrow:-soup-contains(\"City\")>.infobox-label')\n",
    "        area = j.find_next_sibling('td').get_text()\n",
    "        ret_dict['city_size'] = unicodedata.normalize('NFKD',area)\n",
    "\n",
    "    if soup.select_one('.mergedtoprow:-soup-contains(\"Elevation\")>.infobox-data') != None:\n",
    "        k = soup.select_one('.mergedtoprow:-soup-contains(\"Elevation\")>.infobox-data')\n",
    "        elevation_html = k.get_text()\n",
    "        ret_dict['elevation'] = unicodedata.normalize('NFKD',elevation_html)\n",
    "    \n",
    "    if soup.select_one('.mergedtoprow:-soup-contains(\"Population\")') != None:\n",
    "        l = soup.select_one('.mergedtoprow:-soup-contains(\"Population\")')\n",
    "        c_pop = l.findNext('td').get_text()\n",
    "        ret_dict['city_population'] = c_pop\n",
    "    \n",
    "    if soup.select_one('.infobox-label>[title^=Urban]') != None:\n",
    "        m = soup.select_one('.infobox-label>[title^=Urban]')\n",
    "        u_pop = m.findNext('td')\n",
    "        ret_dict['urban_population'] = u_pop.get_text()\n",
    "\n",
    "    if soup.select_one('.infobox-label>[title^=Metro]') != None:\n",
    "        n = soup.select_one('.infobox-label>[title^=Metro]')\n",
    "        m_pop = n.findNext('td')\n",
    "        ret_dict['metro_population'] = m_pop.get_text()\n",
    "    \n",
    "    if soup.select_one('.latitude') != None:\n",
    "        o = soup.select_one('.latitude')\n",
    "        ret_dict['lat'] = o.get_text()\n",
    "\n",
    "    if soup.select_one('.longitude') != None:    \n",
    "        p = soup.select_one('.longitude')\n",
    "        ret_dict['long'] = p.get_text()\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "### a loop to do web-scraping\n",
    "### to get the results in English, we can add a 'header' to our 'requests.get()' line, bellow + this line:\n",
    "# header = {\"Accept-Language\": \"en-US,en;q=0.5\"}\n",
    "\n",
    "list_of_city_info = []\n",
    "for city in cities:\n",
    "    url = f'https://en.wikipedia.org/wiki/{city}'\n",
    "    web = requests.get(url,'html.parser')\n",
    "    soup = bs(web.content)\n",
    "    list_of_city_info.append(City_info(soup))\n",
    "\n",
    "df_cities = pd.DataFrame(list_of_city_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cities= (\n",
    "    df_cities\n",
    "    .assign(area2 = lambda x: x['city_size'].str.split('km2'))\n",
    "    .assign(area3 = lambda x:[x['area2'][i][0] for i in range(5)])\n",
    "    .filter(['city','elevation','city_population','urban_population','metro_population','lat','long','area3'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'area3':'city_area','index':'city_id','city':'city_name','population':'city_population','lat':'latitude','long':'longitude'})\n",
    "    .assign(city_population = lambda x: x['city_population'].replace(to_replace=\"\\,\", value=\"\", regex=True))\n",
    "    .assign(urban_population = lambda x: x['urban_population'].replace(to_replace=\"\\,\", value=\"\", regex=True))\n",
    "    .assign(urban_population = lambda x: x['urban_population'].str.split('['))\n",
    "    .assign(urban_population = lambda x: [x['urban_population'][i][0] for i in range(5)]) \n",
    "    .assign(urban_population = lambda x: x['urban_population'].str.split(\"\\(\"))\n",
    "    .assign(urban_population = lambda x: [x['urban_population'][i][0] for i in range(5)]) \n",
    "    .assign(metro_population = lambda x: x['metro_population'].replace(to_replace=\"\\,\", value=\"\", regex=True))\n",
    "    .assign(metro_population = lambda x: x['metro_population'].str.split('[')) \n",
    "    .assign(metro_population = lambda x: [x['metro_population'][i][0] for i in range(5)]) \n",
    "    .assign(metro_population = lambda x: x['metro_population'].str.split(\"\\(\"))\n",
    "    .assign(metro_population = lambda x: [x['metro_population'][i][0] for i in range(5)]) \n",
    "    .assign(city_area = lambda x: pd.to_numeric(x['city_area']))\n",
    "    .assign(city_population = lambda x: pd.to_numeric(x['city_population']))\n",
    "    .assign(urban_population = lambda x: pd.to_numeric(x['urban_population']))\n",
    "    .assign(metro_population = lambda x: pd.to_numeric(x['metro_population']))\n",
    "    .assign(municipality_iso_country = lambda x: x['city_name']+ \",DE\") \n",
    ")\n",
    "new_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cities.to_csv('CSV/demographics.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
